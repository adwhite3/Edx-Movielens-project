---
title: "MovieLens Edx Project"
author: "Andrew White"
date: "8/5/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
The MovieLens 10M dataset is a dataset of 10 million movie reviews which I will use to train and create a movie recommendation system using machine learning. The goal of this project is to create a movie recommendation system with a root mean squared error of less than 0.8775. I will start with some data exploration to get a better understanding of the dataset. This data exploration is essential for finding trends in the data which can be exploited to create and improve prediction models. I will use an approach called matrix factorization which uses different effects (or biases as Netflix calls them) to predict which movies are closely associated. After matrix factorization, I will use regularization to try to improve those models. 
```{r message=FALSE, warning=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
```
MovieLens 10M dataset:
https://grouplens.org/datasets/movielens/10m/
http://files.grouplens.org/datasets/movielens/ml-10m.zip
```{r}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")
```
Validation set will be 10% of MovieLens data
```{r message=FALSE, warning=FALSE}
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
```
Make sure userId and movieId in validation set are also in edx set
```{r}
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
```
Add rows removed from validation set back into edx set
```{r}
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
# Data Exploration

Check out the size of the dataset and change the options to show up to 7 digits
```{r}
options(digits = 7)
dim(edx)
```
Look at the ratings
```{r}
length(edx$rating)
hist(edx$rating)
edx%>% group_by(rating)%>%
  summarize(count=n())%>%
  arrange(desc(count))%>%
  head()
mean(edx$rating)
```
How many movies and users?
```{r}
length(unique(edx$title))
length(unique(edx$userId))
```
What is the breakdown by genre?
```{r}
edx %>% separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
```

Look at the most rated movies
```{r}
edx %>% group_by(movieId, title) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
```
# Method/Analysis
Create Test/Training sets within EDX data
```{r}
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

Create RMSE function to measure quality of the prediction
```{r}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

For the first prediction, I can just use the average as our baseline to improve upon
```{r}
mu_hat <- mean(train_set$rating)
mu_hat
naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse
predictions <- rep(2.5, nrow(test_set))
RMSE(test_set$rating, predictions)
```

Create a table named rmse_results to store results so I can compare RMSE results across methods
```{r message=FALSE, warning=FALSE}
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)
```

I see the naive rmse is 1.05990. I can improve this by taking into account some general trends or effects in the data that impact rating.
I saw in the movie rating data that there is an effect (or bias as Netflix calls it) caused by some movies having higher average ratings
Call this b_i and adjust the recommendation system to account for this.
```{r}
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))

predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i

model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```

I see adding the movie effect lowers the RMSE to .94374 which is a nice improvement, but it is still not low enough to be helpful.
As I saw in the user data, there is a trend that some users are harsher than others when it comes to ratings. I will call this trend the user effect (b_u) and use it to try to further reduce the RMSE
```{r}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")

user_avgs <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```

Next I combine both effects to improve the model by redefining predicted_ratings with movie and user effects
```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
```

It dropped all the way down to 0.84313! This is great, but by testing a few other techniques, I might be able to improve the model further. First look at the 10 biggest adjustments are at the top and bottom.

Make a movie_titles dataframe so it is easier to see the effects and titles

```{r}
movie_titles <- edx %>% 
  select(movieId, title) %>%
  distinct()
```

Top 10

```{r message=FALSE}
train_set %>% dplyr::count(movieId) %>% 
  left_join(movie_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Bottom 10

```{r message=FALSE}
train_set %>% dplyr::count(movieId) %>% 
  left_join(movie_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

They are barely viewed movies so lets use regularization to account for the number of reviews

Use a random lambda 3 for now that I can adjust later
```{r}
lambda_i<- 3
mu <- mean(train_set$rating)
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda_i), n_i = n()) 

data_frame(original = movie_avgs$b_i, 
           regularized = movie_reg_avgs$b_i, 
           n = movie_reg_avgs$n_i) %>%
  ggplot(aes(original, regularized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5)
```

Review them after regularizaton 

Top 10
```{r message=FALSE}
train_set %>%
  dplyr::count(movieId) %>% 
  left_join(movie_reg_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Bottom 10
```{r message=FALSE}
train_set %>%
  dplyr::count(movieId) %>% 
  left_join(movie_reg_avgs) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Now the movies with the biggest adjustments have very many ratings. That improved a lot, but lets tune the lambda to get the best regularization

```{r}
lambdas <- seq(0, 10, 0.25)
mu <- mean(train_set$rating)
just_the_sum <- train_set %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())
rmses <- sapply(lambdas, function(l){
  predicted_ratings <- test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
lambdas[which.min(rmses)]
```
I see 2.5 is the best lambda for RMSE so I will redefine movie_reg_avgs with lambda_i=2.25
```{r}
lambda_i<-2.5
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda_i), n_i = n())
```
Now check the RMSE of the regularized Movie Effect
```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  .$pred

model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect Model",  
                                     RMSE = model_3_rmse ))
rmse_results %>% knitr::kable()
```

It's 0.94367 which is only a tiny improvement over the unregularized model 1. Perhaps regularization makes a larger improvement to model 2.

I'll look for the optimal lambda for user effect and movie effect and use that to find the RMSE for the regularized combined user and movie effect
```{r}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)  

lambda_iu <- lambdas[which.min(rmses)]
lambda_iu
```

The best lambda for movie and user effects is 4.75 which gives use an RMSE of 0.865
```{r}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User Effect Model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

With that lambda I get a RMSE of 0.86524 which is much better than what I started with, but not an improvement over model 2

# Results

```{r}
rmse_results
```

The combined user and movie effect dropped the RMSE all the way down to 0.84313 which was the best RMSE I achieved on the test set.
```{r}
mu_fin<-mean(edx$rating)

b_i <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu_fin))

b_u <- edx %>% 
  left_join(b_i, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu_fin - b_i))

predicted_ratings_fin <- validation %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  mutate(pred = mu_fin + b_i + b_u) %>%
  .$pred

model_2_rmse_fin <- RMSE(predicted_ratings_fin, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model_fin",  
                                     RMSE = model_2_rmse_fin ))
rmse_results
```

The resulting RMSE of 0.86534 is higher than expected. The RMSE was lower in the training data likely because of overtraining in the test data. The code for the final model using regularized user and movie effects is below. 
```{r}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(edx$rating)
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  predicted_ratings <- 
    validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  return(RMSE(predicted_ratings, validation$rating))
})

qplot(lambdas, rmses)  

lambda_iu <- lambdas[which.min(rmses)]
lambda_iu

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User Effect Model_fin",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

Now the RMSE is 0.86481 which is even better.This improved the RMSE by 0.19508 which means that the movie recommendations will be 19.5% more accurate than simply guessing the average.

# Conclusion
The final RMSE of 0.86481 is a significant improvement over the naive estimate I started with. However there is always room for improvement. For further study, I could look for other effects in the data such as genre and see how that improves the prediction. In future models, care must be taken to not overtrain the model as that will give a false sense of how strong the prediction model is. While regularization did not offer an improvement over in the training set, it did improve on the validation set. This final RMSE is below the 0.8649 threshold, which meets the Netflix challenge.